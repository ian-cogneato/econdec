{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "date = datetime.today().strftime('%y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _utils.clean import normalize, normalize_frame, exclude_3sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import derivatives_dir as derivs_dir\n",
    "bins_dir = derivs_dir / '04.binarized'\n",
    "scales_dir = derivs_dir / 'qualtrics' / '2.subscaled'\n",
    "output_dir = derivs_dir / '05.subject-level'\n",
    "if not Path.exists(output_dir): Path.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fname = bins_dir / ('econdec-all_task-all_' + date + '.csv')\n",
    "behav_data = pd.read_csv(fname).rename(columns={'subjnum':'ssid'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behav_data.columns = [col.replace('-','_') for col in behav_data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = scales_dir / 'all_subscales.csv'\n",
    "scale_data = pd.read_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behav_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scale_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize PANAS and BISBAS subscales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (scale_data.copy(deep=True)\n",
    "          .rename(columns={\n",
    "    'ZIP':'zip', 'Relative Sleep':'sleep', 'Relative Stress':'stress', 'Financial Difficulty':'fin_dif',\n",
    "    'BAS Drive':'BAS_dr', 'BAS Fun Seeking':'BAS_fs', 'BAS Reward Responsiveness':'BAS_rr', \n",
    "    'Intuitive DMS': 'DMS_i', 'Rational DMS':'DMS_r', 'Dependent DMS':'DMS_d',\n",
    "    'Spontaneous DMS':'DMS_s', 'Avoidant DMS':'DMS_a', 'Financial Literacy':'fin_lit'\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['study'] = df['ssid'].astype(str).str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.loc[df['study'] == '1']\n",
    "df2 = df.loc[df['study'] == '2']\n",
    "df3 = df.loc[df['study'] == '3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_normalize = ['PAS','NAS','BIS','BAS_fs','BAS_rr','BAS_dr',\n",
    "                     'DMS_i','DMS_r','DMS_d','DMS_s','DMS_a',]\n",
    "z_cols = ['z_'+col for col in cols_to_normalize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(df): return (df-df.mean()) / df.std(ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[z_cols] = df1.loc[:,cols_to_normalize].apply(z_score)\n",
    "df2[z_cols] = df2.loc[:,cols_to_normalize].apply(z_score)\n",
    "df3[z_cols] = df3.loc[:,cols_to_normalize].apply(z_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.concat([df1,df2,df3]).sort_values('ssid')\n",
    "output_df.sample(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Logarithm of subscales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the natural log of each subscale's RAW score.\n",
    "\n",
    "*NOT* their normalized score, because we can't take the log of a negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_cols = ['ln_'+col for col in cols_to_normalize]\n",
    "output_df[ln_cols] = output_df[cols_to_normalize].applymap(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.sample(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3sd trial exclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _utils.transform import group_exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behav_data['valError_3sd'] = group_exclude(behav_data, group_col = 'ssid', value_col = 'val_estdiff_valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gender-judgment trial exclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have the gender judgment data in the source file for this notebook, but this is where I want to exclude trials in which the gender judgment was wrong. Have to go further upstream to include the gender judgment in this source file, then apply the gender judgment exclusion here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subject-level means"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "optimal_choice_freq = df.groupby('ssid').mean().reset_index()[['ssid','waschoiceoptimal']]\n",
    "optimal_choice_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_means = (behav_data.groupby(['ssid','domain'], as_index=False)\n",
    "                        .mean())\n",
    "domain_means.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gain_ave_val_error = domain_means[domain_means['domain'] == 'GAIN'][['ssid','valError_3sd']]\n",
    "gain_ave_val_error = gain_ave_val_error.set_index('ssid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_ave_val_error = domain_means[domain_means['domain'] == 'LOSS'][['ssid','valError_3sd']]\n",
    "loss_ave_val_error = loss_ave_val_error.set_index('ssid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framing Normalization\n",
    "We want to normalize for the way the value estimation question is framed.\n",
    "\n",
    "We're going to multiply valError means by `1` for subjects who were estimating the probability that the stock is *good*, and multiply means by `-1` for subjects who were estimating the probability that the stock is *bad*.\n",
    "\n",
    "100s: `*  1`\n",
    "\n",
    "200s: `* -1`\n",
    "\n",
    "300s: `*  1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_df = behav_data.groupby('ssid').mean()[['valError_3sd']]\n",
    "means_df[85:91]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf_valerror = pd.DataFrame(means_df.apply(normalize_frame,axis=1))\n",
    "nf_valerror = nf_valerror.rename(columns={0:'nf_valError'})\n",
    "nf_valerror[85:91]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_ave_val_error[85:91]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf_valerr_gain = pd.DataFrame(gain_ave_val_error.apply(normalize_frame,axis=1))\n",
    "nf_valerr_gain = nf_valerr_gain.rename(columns={0:'nf_gainValError'})\n",
    "nf_valerr_gain[85:91]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ave_val_error[85:91]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf_valerr_loss = pd.DataFrame(loss_ave_val_error.apply(normalize_frame,axis=1))\n",
    "nf_valerr_loss = nf_valerr_loss.rename(columns={0:'nf_lossValError'})\n",
    "nf_valerr_loss[85:91]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Output DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = (\n",
    "    output_df.merge(behav_data.groupby('ssid', as_index=False)\n",
    "                       .mean()[['ssid','valError_3sd']])\n",
    "             .merge(gain_ave_val_error.reset_index()\n",
    "                       .rename(columns={'valError_3sd':'gainValError'}))\n",
    "             .merge(loss_ave_val_error.reset_index()\n",
    "                       .rename(columns={'valError_3sd':'lossValError'}))\n",
    "             .merge(nf_valerror.reset_index())\n",
    "             .merge(nf_valerr_gain.reset_index())\n",
    "             .merge(nf_valerr_loss.reset_index())\n",
    "             #.merge(optimal_choice_freq).rename(columns={'waschoiceoptimal':'optimal_choice_freq'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df['valWedge'] = abs(output_df['gainValError'] - output_df['lossValError'])\n",
    "output_df['nf_valWedge'] = abs(output_df['nf_gainValError'] - output_df['nf_lossValError'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = output_dir / ('subject-level_' +date +'.csv')\n",
    "#columns = ['ssid','valError','gainValError','lossValError','valWedge'] + zkeys\n",
    "output_df.to_csv(fname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legacy output dataframe build below\n",
    "\n",
    "Can be ignored."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "output = output.merge(optimal_choice_freq).rename(columns={'waschoiceoptimal':'optimal_choice_freq'})\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "output = output.merge(gain_ave_val_error.rename(columns={'valError_3sd':'gainValError'}).reset_index())\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "output = output.merge(loss_ave_val_error.rename(columns={'valError_3sd':'lossValError'}).reset_index())\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "output = output.merge(nf_valerror.reset_index())\n",
    "output = output.merge(nf_valerr_gain.reset_index())\n",
    "output = output.merge(nf_valerr_loss.reset_index())\n",
    "output[85:91]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "output['valWedge'] = abs(output['gainValError'] - output['lossValError'])\n",
    "output['nf_valWedge'] = abs(output['nf_gainValError'] - output['nf_lossValError'])\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "output = output.merge(norms)\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "output = output.merge(logs)\n",
    "output.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
