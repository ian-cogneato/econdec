{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realign & Homogenize ED-1, ED-2, ED-3\n",
    "\n",
    "`2.clean_realign_homogenize_all`\n",
    "\n",
    "Realign and merge converted data from ED-3 into the concatenated data from ED-1 and ED-2.\n",
    "\n",
    "Differentiate, import, and reassociate memory data into the main-task trialwise dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from _utils import clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.today().strftime('%y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import derivatives_dir as derivs_dir\n",
    "allsub_dir = derivs_dir / '00.allsub'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Concatenated Taskwise data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ED-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_fpath_1 = allsub_dir / ('econdec-1_task-main_beh_' + date + '.csv')\n",
    "frac_fpath_1 = allsub_dir / ('econdec-1_task-frac_beh_' + date + '.csv')\n",
    "face_fpath_1 = allsub_dir / ('econdec-1_task-face_beh_' + date + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df_1 = clean.smooth_columns(pd.read_csv(main_fpath_1))\n",
    "frac_df_1 = clean.smooth_columns(pd.read_csv(frac_fpath_1))\n",
    "face_df_1 = clean.smooth_columns(pd.read_csv(face_fpath_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ED-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_fpath_2 = allsub_dir / ('econdec-2_task-main_beh_' + date + '.csv')\n",
    "frac_fpath_2 = allsub_dir / ('econdec-2_task-frac_beh_' + date + '.csv')\n",
    "face_fpath_2 = allsub_dir / ('econdec-2_task-face_beh_' + date + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df_2 = clean.smooth_columns(pd.read_csv(main_fpath_2))\n",
    "frac_df_2 = clean.smooth_columns(pd.read_csv(frac_fpath_2))\n",
    "face_df_2 = clean.smooth_columns(pd.read_csv(face_fpath_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ED-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_fpath_3 = allsub_dir / ('econdec-3_task-main_beh_' + date + '.csv')\n",
    "frac_fpath_3 = allsub_dir / ('econdec-3_task-frac_beh_' + date + '.csv')\n",
    "face_fpath_3 = allsub_dir / ('econdec-3_task-face_beh_' + date + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df_3 = clean.eye_cleanup(clean.smooth_columns(pd.read_csv(main_fpath_3)))\n",
    "frac_df_3 = clean.smooth_columns(pd.read_csv(frac_fpath_3))\n",
    "face_df_3 = clean.smooth_columns(pd.read_csv(face_fpath_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll address the overall trial count by invoking `eye_cleanup`, a specially designed utility function for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean.eye_cleanup.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "I'm unsure whether the above (repetitive), or one of the options below (unintuitive) is a cleaner way to represent the data corpus at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Still pretty repetitive here, but readable. Sets up better code efficiency later. (I'm leaning towards this option)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data = {\n",
    "    'main':{\n",
    "        1:smooth_columns(pd.read_csv(allsub_dir / ('econdec-1_task-main_beh_' + date + '.csv'))),\n",
    "        2:smooth_columns(pd.read_csv(allsub_dir / ('econdec-2_task-main_beh_' + date + '.csv'))),\n",
    "        3:smooth_columns(pd.read_csv(allsub_dir / ('econdec-3_task-main_beh_' + date + '.csv'))),\n",
    "    },\n",
    "    'frac':{\n",
    "        1:smooth_columns(pd.read_csv(allsub_dir / ('econdec-1_task-frac_beh_' + date + '.csv'))),\n",
    "        2:smooth_columns(pd.read_csv(allsub_dir / ('econdec-2_task-frac_beh_' + date + '.csv'))),\n",
    "        3:smooth_columns(pd.read_csv(allsub_dir / ('econdec-3_task-frac_beh_' + date + '.csv'))),\n",
    "    },\n",
    "    'face':{\n",
    "        1:smooth_columns(pd.read_csv(allsub_dir / ('econdec-1_task-face_beh_' + date + '.csv'))),\n",
    "        2:smooth_columns(pd.read_csv(allsub_dir / ('econdec-2_task-face_beh_' + date + '.csv'))),\n",
    "        3:smooth_columns(pd.read_csv(allsub_dir / ('econdec-3_task-face_beh_' + date + '.csv'))),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Harder to read, better code efficiency *now and later*."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data = {}\n",
    "for task in ['main', 'frac', 'face']:\n",
    "    data[task] = {}\n",
    "    for ds in [1, 2, 3]:\n",
    "        data[task][ds] = smooth_columns(pd.read_csv(\n",
    "            allsub_dir / ('econdec-' + str(ds) + '_task-' + task + '_beh_' + date + '.csv')\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homogenize main task column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_df_1 = main_df_1.rename(columns = new_columns).set_index(['subjnum','block','trial']).reset_index()\n",
    "main_df_2 = main_df_2.rename(columns = new_columns).set_index(['subjnum','block','trial']).reset_index()\n",
    "main_df_3 = main_df_3.rename(columns = new_columns).set_index(['subjnum','block','trial']).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exclude bad subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import exclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df_1 = main_df_1[~main_df_1['subjnum'].isin(exclusions)]\n",
    "main_df_2 = main_df_2[~main_df_2['subjnum'].isin(exclusions)]\n",
    "main_df_3 = main_df_3[~main_df_3['subjnum'].isin(exclusions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    len(main_df_1.subjnum.unique()),\n",
    "    len(main_df_2.subjnum.unique()),\n",
    "    len(main_df_3.subjnum.unique()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial Counts"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trials=[]\n",
    "for s in range(len(main_df_all.subjnum.unique())):\n",
    "    for t in range(1,73):\n",
    "        trials.append(t)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "blocks=[]\n",
    "for s in range(len(main_df_all.subjnum.unique())):\n",
    "    for b in range(1,13):\n",
    "        for x in range(6):\n",
    "            blocks.append(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check size\n",
    "Final merged DataFrame compared to expected number of blocks & trials:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(len(blocks))\n",
    "print(len(trials))\n",
    "print(len(main_df_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 ED3 subjects are missing a trial so the trial and block numbers won't match up here."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "assert len(blocks) == len(trials)\n",
    "assert len(trials) == len(main_df_all)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "main_df_all['trial'] = pd.Series(trials)\n",
    "main_df_all['block'] = pd.Series(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df_1.groupby('subjnum').count().iloc[:,0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df_2.groupby('subjnum').count().iloc[:,0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df_3.groupby('subjnum').count().iloc[:,0].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 problems with the behavioral data from ED-3 above:\n",
    "\n",
    "1. Subjects should each have 72 trials, but we see 90 for most trials. This is for two reasons:\n",
    "    1. Practice blocks are not separate from the Main task in ED-3, adding an additional 2 blocks of 2 trials (total of 4, bringing 72 up to 76)\n",
    "    2. Each block in the Practice and Main tasks comes with an additional trial row, repeating the data from the last trial in that block. This adds 12 rows for the Main task, and 2 for the Practice task (total of 14, bringing our 76 up to 90).\n",
    "2. Some (8) subjects are missing exactly 1 of those 90 trials.\n",
    "    1. We aren't exactly sure why this data was lost, but the EyeLink software seems to have failed to write it into the raw data during some instances of required recalibration of the eye-tracking sensor system. In any case, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we still have to deal with the subjects who are missing trials. Invoking `eye_cleanup` seems to have cleared up the trial count discrepancy for 2 of these 8 subjects, leaving us with 6. This is curious, as it implies that the missing trials for those 2 were either practice trials or the extraneous block-repeat trial rows. We need to work to clarify why this is.\n",
    "\n",
    "In any case, the issue with the remaining 6 subjects can be remedied by adding a \"dummy\" trial in the position of the missing one."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "main_df_3['trial'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the 6 missing trials are all in the 1st trial position within their block, judging by the 6 missing trial values at 1."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "counts = main_df_3.groupby('subjnum').count()['trial']\n",
    "print(counts[counts == 71])\n",
    "subs_missing_trials = counts[counts == 71].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's our 6 offenders listed."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "main_df_3['trialnum'] = pd.concat([\n",
    "    main_df_3.groupby('subjnum').get_group(sub).reset_index().apply(\n",
    "        lambda row: ((int(row.name) // 6) * 6) + row['trial'], axis=1)\n",
    "    for sub in main_df_3['subjnum'].unique()\n",
    "]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "main_df_3['trialnum']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `['study']` label for each DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df_1['study'] = main_df_1.apply(clean.label_study, axis=1)\n",
    "main_df_2['study'] = main_df_2.apply(clean.label_study, axis=1)\n",
    "main_df_3['study'] = main_df_3.apply(clean.label_study, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put `choicert` and `outcomert` in the same units as ED-1 and ED-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ('choicert','outcomert'):\n",
    "    main_df_3[col] = main_df_3[col].astype(float) * .001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaned Output with Exclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusions_dir = derivs_dir / '01.exclusions'\n",
    "if not Path.exists(exclusions_dir): Path.mkdir(exclusions_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df_1.to_csv(exclusions_dir / ('econdec-1_task-main_beh_' + date + '.csv'))\n",
    "main_df_2.to_csv(exclusions_dir / ('econdec-2_task-main_beh_' + date + '.csv'))\n",
    "main_df_3.to_csv(exclusions_dir / ('exondec-3_task-main_beh_' + date + '.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ED-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_df_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ED-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ED-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unified columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_df_all = pd.concat([main_df_1, main_df_2, main_df_3], sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_df_all['stockchosen'] = main_df_all.apply(clean.clean_stockchosen, axis=1)\n",
    "main_df_all['bondpic'] = main_df_all['bondpic'].map(clean.clean_fpath)\n",
    "main_df_all['stockpic'] = main_df_all['stockpic'].map(clean.clean_fpath)\n",
    "len(main_df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fractal Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ED-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_df_1['oldfractal'] = frac_df_1['oldfractal'].map(lambda x : Path(x).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frac_lil_df_1 = frac_df_1[['subjectid','oldfractal','judgment']].sort_values(['subjectid','oldfractal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frac_lil_bond_df_1 = frac_lil_df_1.rename(columns={\n",
    "    'subjectid':'subjnum','oldfractal':'bondpic','judgment':'bondmem'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frac_lil_stock_df_1 = frac_lil_df_1.rename(columns={\n",
    "    'subjectid':'subjnum','oldfractal':'stockpic','judgment':'stockmem'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ED-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_df_2['oldfractal'] = frac_df_1['oldfractal'].map(lambda x : Path(x).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frac_lil_df_2 = frac_df_2[['subjectid','oldfractal','judgment']].sort_values(['subjectid','oldfractal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frac_lil_bond_df_2 = frac_lil_df_2.rename(columns={\n",
    "    'subjectid':'subjnum','oldfractal':'bondpic','judgment':'bondmem'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frac_lil_stock_df_2 = frac_lil_df_2.rename(columns={\n",
    "    'subjectid':'subjnum','oldfractal':'stockpic','judgment':'stockmem'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ED-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_lil_df_3 = frac_df_3[['originalparticipant','correctfractal','selection','correctfractallocation']]\n",
    "frac_lil_df_3['selection'] = frac_lil_df_3.apply(clean.clean_selection, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_lil_bond_df_3 = frac_lil_df_3.rename(columns={\n",
    "    'originalparticipant':'subjnum',\n",
    "    'correctfractal':'bondpic',\n",
    "    'selection':'bondmem'\n",
    "}).drop(columns='correctfractallocation')\n",
    "\n",
    "frac_lil_stock_df_3 = frac_lil_df_3.rename(columns={\n",
    "    'originalparticipant':'subjnum',\n",
    "    'correctfractal':'stockpic',\n",
    "    'selection':'stockmem'\n",
    "}).drop(columns='correctfractallocation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate ED-1, ED-2, ED-3 Fractal Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_lil_bond_df = pd.concat([\n",
    "    frac_lil_bond_df_1, frac_lil_bond_df_2, frac_lil_bond_df_3\n",
    "])\n",
    "\n",
    "frac_lil_stock_df = pd.concat([\n",
    "    frac_lil_stock_df_1, frac_lil_stock_df_2, frac_lil_stock_df_3\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ED-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_lil_df_1 = face_df_1[['subjectid','face','subjresp']]\n",
    "face_lil_df_1 = face_lil_df_1.rename(columns={\n",
    "    'subjectid':'subjnum','face':'facepic','subjresp':'facemem'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ED-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_lil_df_2 = face_df_2[['subjectid','face','subjresp']]\n",
    "face_lil_df_2 = face_lil_df_2.rename(columns={\n",
    "    'subjectid':'subjnum','face':'facepic','subjresp':'facemem'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ED-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_lil_df_3 = face_df_3[\n",
    "    ['originalparticipant','facefile','selection']\n",
    "].rename(columns={\n",
    "    'originalparticipant':'subjnum',\n",
    "    'facefile':'facepic',\n",
    "    'selection':'facemem'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate ED-1, ED-2, ED-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_lil_df = pd.concat([\n",
    "    face_lil_df_1, face_lil_df_2, face_lil_df_3\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reintroduce contextual memory data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df_all = main_df_all.merge(frac_lil_bond_df, how='left')\n",
    "main_df_all = main_df_all.merge(frac_lil_stock_df, how='left')\n",
    "main_df_all = main_df_all.merge(face_lil_df, how='left')\n",
    "# unified_main_frame[['subjnum','stockpic','bondpic','stockmem','bondmem']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Unnecessary Columns?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_1_drop_columns = [\n",
    "    'agegroup','experimentername','date','time','trialnumbydomdist',\n",
    "    'choicest','outcomest','esttaskst',\n",
    "    'confidencest','stocknumber','bondnumber','genderjudgment',\n",
    "    'fractalchosen','estwithinrange?','confidencert'\n",
    "]\n",
    "df_2_drop_columns = df_1_drop_columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_3_drop_columns = [\n",
    "    'practice','bubblefile','bondvalue','stocktext','bondtext',\n",
    "    'stocktextlocation','bondtextlocation','emotionresponse','bypassed','agegroup','experimentername',\n",
    "    'date','correctfractallocation','incorrectfractallocation','paymentaccuracy','phase',\n",
    "    'stockfractallocation','bondfractallocation','stockfractallocationtype','bondfractallocationtype',\n",
    "    'showinstruction','gender','selection',\n",
    "    'correctfractal','incorectfractal','oldfaceequalstrue','facefile','facekeypressed',\n",
    "    'originalsubjectnumber','originalparticipantnumber','originaltrialnumber','originaltrailnumber',\n",
    "    'fracdomain','facedomain','fracmagnitude','facestockvalue','genderjudgment',\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "main_df_1 = main_df_1.drop(df_1_drop_columns, axis=1)\n",
    "main_df_2 = main_df_2.drop(df_2_drop_columns, axis=1)\n",
    "main_df_3 = main_df_3.drop(df_3_drop_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_drop_columns = [\n",
    "    'agegroup','experimentername','date','time','trialnumbydomdist',\n",
    "    'choicest','outcomest','esttaskst',\n",
    "    'confidencest','stocknumber','bondnumber','genderjudgment',\n",
    "    'fractalchosen','estwithinrange?','confidencert',\n",
    "    'practice','bubblefile','bondvalue','stocktext','bondtext',\n",
    "    'stocktextlocation','bondtextlocation','emotionresponse','bypassed','agegroup','experimentername',\n",
    "    'date','correctfractallocation','incorrectfractallocation','paymentaccuracy','phase',\n",
    "    'stockfractallocation','bondfractallocation','stockfractallocationtype','bondfractallocationtype',\n",
    "    'showinstruction','gender','selection','cueonleft','cueonright',\n",
    "    'correctfractal','incorectfractal','oldfaceequalstrue','facefile','facekeypressed',\n",
    "    'originalsubjectnumber','originalparticipantnumber','originaltrialnumber','originaltrailnumber',\n",
    "    'fracdomain','facedomain','fracmagnitude','facestockvalue','genderjudgment',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not using any of the columns listed above. There's no real reason to remove the data, but it makes the output cleaner and easier to look at without all the extraneous information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df_all = main_df_all.drop(df_all_drop_columns, axis=1).set_index([\n",
    "    'subjnum','block','trial'\n",
    "]).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output\n",
    "\n",
    "ONly when all data is fully aligned and homogenized.\n",
    "\n",
    "**ALL** cleaning steps should be done before this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homog_dir = derivs_dir / '02.homogenized'\n",
    "if not Path.exists(homog_dir):\n",
    "    Path.mkdir(homog_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = homog_dir  / ('econdec-full_task-main_beh_' + date + '.csv')\n",
    "main_df_all.to_csv(fpath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(main_df_all.subjnum.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference:\n",
    "\n",
    "```\n",
    "final_columns=['study','subjnum','trial','block','domain','dom',\n",
    "               'estimation','trueprob','estdiff','valestdiff','valestdiffvalid',\n",
    "               'choicert','choicerta3sd','choicerti3sd','choicemed12v3','choicemed123'\n",
    "               'esttaskrt','esttaskrta3sd','esttaskrti3sd',\n",
    "               'outcomert','outcomerta3sd','outcomerti3sd','outcomemed12','outcomemed123'\n",
    "               'stockchosen','waschoiceoptimal','optimalchoiceshouldhavebeen',\n",
    "               'magnitude','stockvalue','absstockval','b4choiceprobability',\n",
    "               'stockpic','bondpic','facepic','stockmemresp','bondmemresp',\n",
    "               'studymedchoice','studysplitchoice','studymedoutcome','studysplitoutcome',\n",
    "               'primemedchoice','primesplitchoice','primemedoutcome','primesplitoutcome']\n",
    "               ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
